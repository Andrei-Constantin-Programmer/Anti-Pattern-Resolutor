{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b669f3",
   "metadata": {},
   "source": [
    "# Java Antipattern Scanner - Refactored Demo\n",
    "\n",
    "This is a refactored Java antipattern scanner demonstration, based on a modular codebase structure.\n",
    "\n",
    "## Key improvements:\n",
    "\n",
    "- **Modular structure**: Clear separation of agents, workflow, and data layers\n",
    "- **English documentation**: All comments and docstrings translated to English\n",
    "- **Enhanced workflow**: Multi-step analysis pipeline with LLM integration\n",
    "- **Better error handling**: Comprehensive exception handling and fallbacks\n",
    "- **Standardized configuration**: Centralized settings management\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "This demo will create the following modular structure:\n",
    "\n",
    "```\n",
    "java_antipatterns_scanner/\n",
    "â”œâ”€â”€ __init__.py                    # Package initialization\n",
    "â”œâ”€â”€ config.py                      # Configuration management\n",
    "â”œâ”€â”€ requirements.txt               # Dependency management\n",
    "â”œâ”€â”€ main.py                        # Main entry script\n",
    "â”œâ”€â”€ db/                           # Database module\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â””â”€â”€ vector_db.py              # Vector database management\n",
    "â”œâ”€â”€ analysis/                     # Analysis module\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ retriever.py             # Retrieval tools\n",
    "â”‚   â””â”€â”€ analyzer.py              # Code analyzer\n",
    "â””â”€â”€ workflow/                     # Workflow module\n",
    "    â”œâ”€â”€ __init__.py\n",
    "    â””â”€â”€ langgraph_workflow.py     # LangGraph workflow\n",
    "```\n",
    "\n",
    "## Limitations (To be addressed in future versions)\n",
    "\n",
    "- Query methods: Still using the original code snippets for semantic querying\n",
    "- Agent structure: Current version is still a single-agent architecture\n",
    "- Model support: Only local Ollama model is supported\n",
    "- Usage: Manual code provision for analysis is still required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# å®šä¹‰é¡¹ç›®æ ¹ç›®å½•\n",
    "project_root = Path(\"java_antipatterns_scanner\")\n",
    "\n",
    "# åˆ›å»ºç›®å½•ç»“æ„\n",
    "directories = [\n",
    "    project_root,\n",
    "    project_root / \"db\",\n",
    "    project_root / \"analysis\", \n",
    "    project_root / \"workflow\"\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„...\")\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… åˆ›å»ºç›®å½•: {directory}\")\n",
    "\n",
    "# åˆ›å»º__init__.pyæ–‡ä»¶\n",
    "init_files = [\n",
    "    project_root / \"__init__.py\",\n",
    "    project_root / \"db\" / \"__init__.py\",\n",
    "    project_root / \"analysis\" / \"__init__.py\",\n",
    "    project_root / \"workflow\" / \"__init__.py\"\n",
    "]\n",
    "\n",
    "for init_file in init_files:\n",
    "    init_file.touch()\n",
    "    print(f\"âœ… åˆ›å»ºæ–‡ä»¶: {init_file}\")\n",
    "\n",
    "print(\"\\nğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ!\")\n",
    "print(\"\\nç›®å½•ç»“æ„:\")\n",
    "def print_tree(directory, prefix=\"\", is_last=True):\n",
    "    \"\"\"é€’å½’æ‰“å°ç›®å½•æ ‘\"\"\"\n",
    "    name = directory.name\n",
    "    print(f\"{prefix}{'â””â”€â”€ ' if is_last else 'â”œâ”€â”€ '}{name}/\")\n",
    "    \n",
    "    # è·å–å­é¡¹ç›®å¹¶æ’åº\n",
    "    children = sorted([child for child in directory.iterdir() if child.is_dir()])\n",
    "    files = sorted([child for child in directory.iterdir() if child.is_file()])\n",
    "    \n",
    "    # æ‰“å°å­ç›®å½•\n",
    "    for i, child in enumerate(children):\n",
    "        is_last_child = (i == len(children) - 1) and len(files) == 0\n",
    "        new_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "        print_tree(child, new_prefix, is_last_child)\n",
    "    \n",
    "    # æ‰“å°æ–‡ä»¶\n",
    "    for i, file in enumerate(files):\n",
    "        is_last_file = i == len(files) - 1\n",
    "        print(f\"{prefix}{'â””â”€â”€ ' if is_last else 'â”œâ”€â”€ '}{'â””â”€â”€ ' if is_last_file else 'â”œâ”€â”€ '}{file.name}\")\n",
    "\n",
    "print_tree(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407489f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬äºŒæ­¥ï¼šåˆ›å»ºé…ç½®ç®¡ç†æ¨¡å—\n",
    "config_content = '''\"\"\"\n",
    "é…ç½®ç®¡ç†æ¨¡å— - ç»Ÿä¸€ç®¡ç†é¡¹ç›®é…ç½®å‚æ•°\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    \"\"\"é¡¹ç›®é…ç½®ç±»\"\"\"\n",
    "    \n",
    "    # åŸºç¡€è·¯å¾„é…ç½®\n",
    "    PROJECT_ROOT = Path(__file__).parent\n",
    "    DATA_DIR = PROJECT_ROOT.parent / \"static\"\n",
    "    \n",
    "    # LLMæ¨¡å‹é…ç½®\n",
    "    LLM_MODEL = \"granite3.3:8b\"\n",
    "    EMBEDDING_MODEL = \"nomic-embed-text:v1.5\"\n",
    "    \n",
    "    # å‘é‡æ•°æ®åº“é…ç½®\n",
    "    VECTOR_DB_PATH = os.path.expanduser(\"~/antipattern_vectordb\")\n",
    "    ANTIPATTERN_FILE = DATA_DIR / \"ap.txt\"\n",
    "    \n",
    "    # æ–‡æ¡£åˆ†å‰²é…ç½®\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    \n",
    "    # æ£€ç´¢é…ç½®\n",
    "    RETRIEVAL_K = 4  # æ£€ç´¢ç›¸å…³æ–‡æ¡£æ•°é‡\n",
    "    \n",
    "    @classmethod\n",
    "    def get_antipattern_file_path(cls):\n",
    "        \"\"\"è·å–åæ¨¡å¼æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå¤šç§å¯èƒ½ä½ç½®\"\"\"\n",
    "        possible_paths = [\n",
    "            cls.ANTIPATTERN_FILE,\n",
    "            cls.PROJECT_ROOT.parent / \"static\" / \"ap.txt\",\n",
    "            Path(\"static/ap.txt\"),\n",
    "            Path(\"ap.txt\")\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                return str(path)\n",
    "        \n",
    "        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤è·¯å¾„ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰\n",
    "        return str(cls.ANTIPATTERN_FILE)\n",
    "\n",
    "# å…¨å±€é…ç½®å®ä¾‹\n",
    "config = Config()\n",
    "'''\n",
    "\n",
    "# å†™å…¥é…ç½®æ–‡ä»¶\n",
    "config_file = project_root / \"config.py\"\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"âœ… é…ç½®ç®¡ç†æ¨¡å— (config.py) åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# åˆ›å»ºrequirements.txt\n",
    "requirements_content = '''# LangChainç›¸å…³\n",
    "langchain>=0.1.0\n",
    "langchain-community>=0.0.20\n",
    "langchain-ollama>=0.1.0\n",
    "langgraph>=0.0.30\n",
    "\n",
    "# å‘é‡æ•°æ®åº“\n",
    "chromadb>=0.4.0\n",
    "\n",
    "# å…¶ä»–ä¾èµ–\n",
    "typing-extensions>=4.0.0\n",
    "'''\n",
    "\n",
    "requirements_file = project_root / \"requirements.txt\"\n",
    "with open(requirements_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"âœ… ä¾èµ–é…ç½®æ–‡ä»¶ (requirements.txt) åˆ›å»ºå®Œæˆ\")\n",
    "print(\"\\nğŸ“‹ é…ç½®è¦ç‚¹:\")\n",
    "print(\"- æ”¯æŒå¤šç§åæ¨¡å¼æ–‡ä»¶ä½ç½®è‡ªåŠ¨æ£€æµ‹\")\n",
    "print(\"- ç»Ÿä¸€çš„æ¨¡å‹å’Œè·¯å¾„å‚æ•°ç®¡ç†\")  \n",
    "print(\"- å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®\")\n",
    "print(\"- æ¨¡å—åŒ–çš„é…ç½®ç»“æ„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬ä¸‰æ­¥ï¼šå®ç°å‘é‡æ•°æ®åº“æ¨¡å—\n",
    "vector_db_content = '''\"\"\"\n",
    "å‘é‡æ•°æ®åº“ç®¡ç†æ¨¡å— - è´Ÿè´£æ–‡æ¡£åŠ è½½ã€åˆ†å‰²å’Œå‘é‡åŒ–å­˜å‚¨\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    from langchain_community.document_loaders import TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_ollama import OllamaEmbeddings\n",
    "except ImportError:\n",
    "    # å‘åå…¼å®¹å¯¼å…¥\n",
    "    from langchain.document_loaders import TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from .config import config\n",
    "\n",
    "\n",
    "class VectorDBManager:\n",
    "    \"\"\"å‘é‡æ•°æ®åº“ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, persist_directory: Optional[str] = None):\n",
    "        self.persist_directory = persist_directory or config.VECTOR_DB_PATH\n",
    "        self.vectordb = None\n",
    "        self.is_initialized = False\n",
    "    \n",
    "    def init_vector_db(self, file_path: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‘é‡æ•°æ®åº“\n",
    "        \n",
    "        Args:\n",
    "            file_path: åæ¨¡å¼æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„\n",
    "            \n",
    "        Returns:\n",
    "            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # ç¡®å®šæ–‡ä»¶è·¯å¾„\n",
    "            if file_path is None:\n",
    "                file_path = config.get_antipattern_file_path()\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"ğŸ“ åŠ è½½åæ¨¡å¼æ–‡ä»¶: {file_path}\")\n",
    "            \n",
    "            # åŠ è½½æ–‡æ¡£\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # åˆ†å‰²æ–‡æ¡£\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=config.CHUNK_SIZE,\n",
    "                chunk_overlap=config.CHUNK_OVERLAP,\n",
    "                length_function=len\n",
    "            )\n",
    "            split_docs = text_splitter.split_documents(docs)\n",
    "            print(f\"ğŸ“„ æ–‡æ¡£åˆ†å‰²ä¸º {len(split_docs)} ä¸ªå—\")\n",
    "            \n",
    "            # åˆ›å»ºæŒä¹…åŒ–ç›®å½•\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            \n",
    "            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "            print(f\"ğŸ¤– åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {config.EMBEDDING_MODEL}\")\n",
    "            embedding = OllamaEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "            \n",
    "            # åˆ›å»ºå‘é‡æ•°æ®åº“\n",
    "            self.vectordb = Chroma(\n",
    "                embedding_function=embedding,\n",
    "                persist_directory=self.persist_directory\n",
    "            )\n",
    "            \n",
    "            # æ·»åŠ æ–‡æ¡£\n",
    "            print(\"ğŸ’¾ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...\")\n",
    "            self.vectordb.add_documents(split_docs)\n",
    "            \n",
    "            # æŒä¹…åŒ–\n",
    "            if hasattr(self.vectordb, 'persist'):\n",
    "                self.vectordb.persist()\n",
    "            \n",
    "            self.is_initialized = True\n",
    "            chunk_count = self.get_chunk_count()\n",
    "            print(f\"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ! å­˜å‚¨äº† {chunk_count} ä¸ªæ–‡æ¡£å—\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_existing_db(self) -> bool:\n",
    "        \"\"\"åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.persist_directory):\n",
    "                print(f\"âš ï¸ å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {self.persist_directory}\")\n",
    "                return False\n",
    "            \n",
    "            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "            embedding = OllamaEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "            \n",
    "            # åŠ è½½ç°æœ‰æ•°æ®åº“\n",
    "            self.vectordb = Chroma(\n",
    "                embedding_function=embedding,\n",
    "                persist_directory=self.persist_directory\n",
    "            )\n",
    "            \n",
    "            chunk_count = self.get_chunk_count()\n",
    "            if chunk_count > 0:\n",
    "                self.is_initialized = True\n",
    "                print(f\"âœ… æˆåŠŸåŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {chunk_count} ä¸ªæ–‡æ¡£å—\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âš ï¸ å‘é‡æ•°æ®åº“ä¸ºç©º\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_retriever(self):\n",
    "        \"\"\"è·å–æ£€ç´¢å™¨\"\"\"\n",
    "        if not self.is_initialized or self.vectordb is None:\n",
    "            raise ValueError(\"å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_vector_db() æˆ– load_existing_db()\")\n",
    "        \n",
    "        return self.vectordb.as_retriever(search_kwargs={\"k\": config.RETRIEVAL_K})\n",
    "    \n",
    "    def get_chunk_count(self) -> int:\n",
    "        \"\"\"è·å–æ–‡æ¡£å—æ•°é‡\"\"\"\n",
    "        try:\n",
    "            if self.vectordb and hasattr(self.vectordb, '_collection'):\n",
    "                return self.vectordb._collection.count()\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "\n",
    "def init_vector_db(file_path: Optional[str] = None, force_recreate: bool = False) -> VectorDBManager:\n",
    "    \"\"\"\n",
    "    ä¾¿æ·å‡½æ•°ï¼šåˆå§‹åŒ–å‘é‡æ•°æ®åº“\n",
    "    \n",
    "    Args:\n",
    "        file_path: åæ¨¡å¼æ–‡ä»¶è·¯å¾„\n",
    "        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ›å»º\n",
    "        \n",
    "    Returns:\n",
    "        VectorDBManager: æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹\n",
    "    \"\"\"\n",
    "    db_manager = VectorDBManager()\n",
    "    \n",
    "    # å¦‚æœä¸å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼Œå…ˆå°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“\n",
    "    if not force_recreate and db_manager.load_existing_db():\n",
    "        return db_manager\n",
    "    \n",
    "    # åˆ›å»ºæ–°çš„æ•°æ®åº“\n",
    "    if db_manager.init_vector_db(file_path):\n",
    "        return db_manager\n",
    "    else:\n",
    "        raise RuntimeError(\"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥\")\n",
    "'''\n",
    "\n",
    "# å†™å…¥å‘é‡æ•°æ®åº“æ¨¡å—\n",
    "vector_db_file = project_root / \"db\" / \"vector_db.py\"\n",
    "with open(vector_db_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(vector_db_content)\n",
    "\n",
    "print(\"âœ… å‘é‡æ•°æ®åº“æ¨¡å— (db/vector_db.py) åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# æ›´æ–°dbåŒ…çš„__init__.py\n",
    "db_init_content = '''\"\"\"\n",
    "æ•°æ®åº“æ¨¡å— - å‘é‡æ•°æ®åº“ç®¡ç†\n",
    "\"\"\"\n",
    "\n",
    "from .vector_db import VectorDBManager, init_vector_db\n",
    "\n",
    "__all__ = [\"VectorDBManager\", \"init_vector_db\"]\n",
    "'''\n",
    "\n",
    "db_init_file = project_root / \"db\" / \"__init__.py\"\n",
    "with open(db_init_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(db_init_content)\n",
    "\n",
    "print(\"âœ… æ•°æ®åº“åŒ…åˆå§‹åŒ–æ–‡ä»¶æ›´æ–°å®Œæˆ\")\n",
    "print(\"\\nğŸ“‹ å‘é‡æ•°æ®åº“æ¨¡å—ç‰¹ç‚¹:\")\n",
    "print(\"- æ”¯æŒè‡ªåŠ¨æ£€æµ‹åæ¨¡å¼æ–‡ä»¶ä½ç½®\")\n",
    "print(\"- å¯é‡ç”¨ç°æœ‰å‘é‡æ•°æ®åº“\")\n",
    "print(\"- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡º\")\n",
    "print(\"- çµæ´»çš„é…ç½®å‚æ•°æ”¯æŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬å››æ­¥ï¼šå®ç°æ£€ç´¢ä¸åˆ†ææ¨¡å—\n",
    "\n",
    "# åˆ›å»ºretriever.py - æ£€ç´¢å·¥å…·æ¨¡å—\n",
    "retriever_content = '''\"\"\"\n",
    "æ£€ç´¢å·¥å…·æ¨¡å— - å°è£…æ£€ç´¢ç›¸å…³åŠŸèƒ½\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from langchain.tools.retriever import create_retriever_tool\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "except ImportError:\n",
    "    from langchain.tools.retriever import create_retriever_tool\n",
    "    from langchain.chat_models import ChatOllama\n",
    "\n",
    "from .config import config\n",
    "\n",
    "\n",
    "class RetrieverManager:\n",
    "    \"\"\"æ£€ç´¢ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, vectordb_manager):\n",
    "        self.vectordb_manager = vectordb_manager\n",
    "        self.retriever_tool = None\n",
    "        self.llm = None\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ£€ç´¢ç»„ä»¶\"\"\"\n",
    "        # åˆ›å»ºæ£€ç´¢å™¨\n",
    "        retriever = self.vectordb_manager.get_retriever()\n",
    "        \n",
    "        # åˆ›å»ºæ£€ç´¢å·¥å…·\n",
    "        self.retriever_tool = create_retriever_tool(\n",
    "            retriever,\n",
    "            name=\"retrieve_Java_antipatterns\",\n",
    "            description=\"Search for Java anti-patterns in the codebase\",\n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–LLM\n",
    "        self.llm = ChatOllama(model=config.LLM_MODEL)\n",
    "        print(f\"ğŸ¤– LLMæ¨¡å‹åˆå§‹åŒ–: {self.llm.model}\")\n",
    "    \n",
    "    def get_retriever_tool(self):\n",
    "        \"\"\"è·å–æ£€ç´¢å·¥å…·\"\"\"\n",
    "        return self.retriever_tool\n",
    "    \n",
    "    def get_llm(self):\n",
    "        \"\"\"è·å–LLMå®ä¾‹\"\"\"\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "def create_retriever_components(vectordb_manager):\n",
    "    \"\"\"\n",
    "    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºæ£€ç´¢ç»„ä»¶\n",
    "    \n",
    "    Args:\n",
    "        vectordb_manager: å‘é‡æ•°æ®åº“ç®¡ç†å™¨\n",
    "        \n",
    "    Returns:\n",
    "        RetrieverManager: æ£€ç´¢ç®¡ç†å™¨å®ä¾‹\n",
    "    \"\"\"\n",
    "    return RetrieverManager(vectordb_manager)\n",
    "'''\n",
    "\n",
    "retriever_file = project_root / \"analysis\" / \"retriever.py\"\n",
    "with open(retriever_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(retriever_content)\n",
    "\n",
    "print(\"âœ… æ£€ç´¢æ¨¡å— (analysis/retriever.py) åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# åˆ›å»ºanalyzer.py - ä»£ç åˆ†ææ¨¡å—  \n",
    "analyzer_content = '''\"\"\"\n",
    "ä»£ç åˆ†ææ¨¡å— - å®ç°Javaä»£ç åæ¨¡å¼åˆ†æé€»è¾‘\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class JavaCodeAnalyzer:\n",
    "    \"\"\"Javaä»£ç åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    # åˆ†ææç¤ºæ¨¡æ¿ (ä¸åŸDemoç›¸åŒ)\n",
    "    ANALYSIS_PROMPT = (\n",
    "        \"You are a senior Java code reviewer with deep experience in detecting software design antipatterns. \"\n",
    "        \"Below is the code to analyze:\\\\n\"\n",
    "        \"{code}\\\\n\\\\n\"\n",
    "        \"Here is additional context from the codebase:\\\\n\"\n",
    "        \"{context}\\\\n\\\\n\"\n",
    "        \"Your task is to:\\\\n\"\n",
    "        \"- Carefully analyze the code.\\\\n\"\n",
    "        \"- Identify any Java antipatterns or design smells present.\\\\n\"\n",
    "        \"- For each antipattern you find, include:\\\\n\"\n",
    "        \"  - [Name of the antipattern] (e.g., God Object, Long Method)\\\\n\"\n",
    "        \"  - [File or class/method name involved] (if detectable)\\\\n\"\n",
    "        \"  - [Brief description] of the issue\\\\n\"\n",
    "        \"  - [Why it\\\\'s a problem]\\\\n\"\n",
    "        \"  - [Suggested refactor]\\\\n\"\n",
    "        \"Be thorough but concise. If no antipatterns are found, say so.\"\n",
    "    )\n",
    "    \n",
    "    def __init__(self, retriever_manager):\n",
    "        self.retriever_manager = retriever_manager\n",
    "        self.retriever_tool = retriever_manager.get_retriever_tool()\n",
    "        self.llm = retriever_manager.get_llm()\n",
    "    \n",
    "    def retrieve_context(self, code: str) -> str:\n",
    "        \"\"\"\n",
    "        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "        \n",
    "        Args:\n",
    "            code: Javaä»£ç \n",
    "            \n",
    "        Returns:\n",
    "            str: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ” æ£€ç´¢ç›¸å…³åæ¨¡å¼ä¸Šä¸‹æ–‡...\")\n",
    "            \n",
    "            # åŸºäºä»£ç ç‰‡æ®µåˆ›å»ºæœç´¢æŸ¥è¯¢\n",
    "            search_query = f\"Java antipatterns code analysis: {code[:200]}\"\n",
    "            \n",
    "            # ä½¿ç”¨æ£€ç´¢å·¥å…·è·å–ç›¸å…³ä¸Šä¸‹æ–‡\n",
    "            context = self.retriever_tool.invoke({\"query\": search_query})\n",
    "            \n",
    "            print(\"   âœ… æˆåŠŸæ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡\")\n",
    "            return context\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ä¸Šä¸‹æ–‡æ£€ç´¢é”™è¯¯: {e}\")\n",
    "            return \"No additional context available due to retrieval error.\"\n",
    "    \n",
    "    def analyze_code(self, code: str, context: str = None) -> str:\n",
    "        \"\"\"\n",
    "        åˆ†æJavaä»£ç ä¸­çš„åæ¨¡å¼\n",
    "        \n",
    "        Args:\n",
    "            code: å¾…åˆ†æçš„Javaä»£ç \n",
    "            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "            \n",
    "        Returns:\n",
    "            str: åˆ†æç»“æœ\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ” åˆ†æä»£ç ä¸­çš„åæ¨¡å¼...\")\n",
    "            \n",
    "            # å¦‚æœæ²¡æœ‰æä¾›ä¸Šä¸‹æ–‡ï¼Œåˆ™è‡ªåŠ¨æ£€ç´¢\n",
    "            if context is None:\n",
    "                context = self.retrieve_context(code)\n",
    "            \n",
    "            # æ ¼å¼åŒ–åˆ†ææç¤º\n",
    "            prompt = self.ANALYSIS_PROMPT.format(\n",
    "                code=code,\n",
    "                context=context\n",
    "            )\n",
    "            \n",
    "            # ä½¿ç”¨LLMè¿›è¡Œåˆ†æ\n",
    "            response = self.llm.invoke(prompt)\n",
    "            result = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            print(\"   âœ… ä»£ç åˆ†æå®Œæˆ\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"ä»£ç åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\"\n",
    "            print(f\"   âŒ {error_msg}\")\n",
    "            return error_msg\n",
    "    \n",
    "    def full_analysis(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å®Œæ•´çš„ä»£ç åˆ†ææµç¨‹\n",
    "        \n",
    "        Args:\n",
    "            code: å¾…åˆ†æçš„Javaä»£ç \n",
    "            \n",
    "        Returns:\n",
    "            Dict: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ å¼€å§‹å®Œæ•´çš„Javaåæ¨¡å¼åˆ†æ...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # æ£€ç´¢ä¸Šä¸‹æ–‡\n",
    "        context = self.retrieve_context(code)\n",
    "        \n",
    "        # æ‰§è¡Œåˆ†æ\n",
    "        analysis_result = self.analyze_code(code, context)\n",
    "        \n",
    "        # è¿”å›ç»“æ„åŒ–ç»“æœ\n",
    "        result = {\n",
    "            \"code\": code,\n",
    "            \"context\": context,\n",
    "            \"analysis\": analysis_result,\n",
    "            \"success\": \"Error\" not in analysis_result\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ‰ åˆ†æå®Œæˆ!\")\n",
    "        return result\n",
    "\n",
    "\n",
    "def create_analyzer(retriever_manager):\n",
    "    \"\"\"\n",
    "    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºä»£ç åˆ†æå™¨\n",
    "    \n",
    "    Args:\n",
    "        retriever_manager: æ£€ç´¢ç®¡ç†å™¨\n",
    "        \n",
    "    Returns:\n",
    "        JavaCodeAnalyzer: ä»£ç åˆ†æå™¨å®ä¾‹\n",
    "    \"\"\"\n",
    "    return JavaCodeAnalyzer(retriever_manager)\n",
    "'''\n",
    "\n",
    "analyzer_file = project_root / \"analysis\" / \"analyzer.py\"\n",
    "with open(analyzer_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(analyzer_content)\n",
    "\n",
    "print(\"âœ… åˆ†ææ¨¡å— (analysis/analyzer.py) åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# æ›´æ–°analysisåŒ…çš„__init__.py\n",
    "analysis_init_content = '''\"\"\"\n",
    "åˆ†ææ¨¡å— - ä»£ç æ£€ç´¢å’Œåæ¨¡å¼åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "from .retriever import RetrieverManager, create_retriever_components\n",
    "from .analyzer import JavaCodeAnalyzer, create_analyzer\n",
    "\n",
    "__all__ = [\n",
    "    \"RetrieverManager\", \n",
    "    \"create_retriever_components\",\n",
    "    \"JavaCodeAnalyzer\", \n",
    "    \"create_analyzer\"\n",
    "]\n",
    "'''\n",
    "\n",
    "analysis_init_file = project_root / \"analysis\" / \"__init__.py\"\n",
    "with open(analysis_init_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(analysis_init_content)\n",
    "\n",
    "print(\"âœ… åˆ†æåŒ…åˆå§‹åŒ–æ–‡ä»¶æ›´æ–°å®Œæˆ\")\n",
    "print(\"\\nğŸ“‹ åˆ†ææ¨¡å—ç‰¹ç‚¹:\")\n",
    "print(\"- åˆ†ç¦»æ£€ç´¢å’Œåˆ†æé€»è¾‘\")\n",
    "print(\"- æ”¯æŒç‹¬ç«‹çš„ä¸Šä¸‹æ–‡æ£€ç´¢\")\n",
    "print(\"- ç»“æ„åŒ–çš„åˆ†æç»“æœè¿”å›\")\n",
    "print(\"- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†æœºåˆ¶\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
